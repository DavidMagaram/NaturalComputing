{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Intrusion Detection for Unix Processes\n",
    "\n",
    "Apply the negative selection algorithm to detect anomalous system call sequences.\n",
    "\n",
    "**Datasets:** `snd-cert` and `snd-unm` (in `syscalls/`)\n",
    "\n",
    "**Key differences from Exercise 1:**\n",
    "- Sequences are variable-length (up to ~1000 chars vs fixed 10 in the language example)\n",
    "- Labels are in separate `.labels` files (0 = normal, 1 = anomalous)\n",
    "- Must specify `-alphabet file://<alpha_file>` since the alphabet differs from the training data\n",
    "\n",
    "**Approach:** Pre-process variable-length sequences into fixed-length chunks of size `n`\n",
    "(non-overlapping to keep runtime manageable). For each test sequence, chunk it, score each chunk\n",
    "with negsel, and average the chunk scores into a composite anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".\")\n",
    "NEGSEL_JAR = BASE_DIR / \"negsel2.jar\"\n",
    "SYSCALLS_DIR = BASE_DIR / \"syscalls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str):\n",
    "    \"\"\"Load a syscalls dataset (e.g. 'snd-cert' or 'snd-unm').\"\"\"\n",
    "    d = SYSCALLS_DIR / name\n",
    "    train_file = d / f\"{name}.train\"\n",
    "    alpha_file = d / f\"{name}.alpha\"\n",
    "    train_seqs = train_file.read_text().strip().split(\"\\n\")\n",
    "    \n",
    "    test_sets = []\n",
    "    for i in [1, 2, 3]:\n",
    "        test_seqs = (d / f\"{name}.{i}.test\").read_text().strip().split(\"\\n\")\n",
    "        labels = [int(x) for x in (d / f\"{name}.{i}.labels\").read_text().strip().split(\"\\n\")]\n",
    "        test_sets.append((test_seqs, labels))\n",
    "    \n",
    "    return train_seqs, alpha_file, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"snd-cert\", \"snd-unm\"]:\n",
    "    train, alpha, tests = load_dataset(name)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"  Alphabet: {alpha.read_text().strip()[:60]}\")\n",
    "    print(f\"  Training sequences: {len(train)}\")\n",
    "    train_lens = [len(s) for s in train]\n",
    "    print(f\"  Train seq lengths: min={min(train_lens)}, max={max(train_lens)}, median={int(np.median(train_lens))}\")\n",
    "    for i, (seqs, labels) in enumerate(tests, 1):\n",
    "        n_anom = sum(labels)\n",
    "        seq_lens = [len(s) for s in seqs]\n",
    "        print(f\"  Test {i}: {len(seqs)} seqs ({n_anom} anomalous, {len(seqs)-n_anom} normal), \"\n",
    "              f\"lengths min={min(seq_lens)} max={max(seq_lens)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Chunking\n",
    "\n",
    "Split variable-length sequences into non-overlapping fixed-length chunks of size `n`.\n",
    "Remainders shorter than `n` are discarded.\n",
    "\n",
    "**Why non-overlapping?** Overlapping (stride=1) on ~1000-char sequences produces ~990 chunks each.\n",
    "With 800+ training sequences that's ~800K self strings for negsel â€” way too slow.\n",
    "Non-overlapping (stride=`n`) gives ~100 chunks per sequence (for `n=10`), keeping runtime under a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sequence(seq: str, n: int) -> list[str]:\n",
    "    \"\"\"Split a sequence into non-overlapping chunks of length n.\"\"\"\n",
    "    return [seq[i:i+n] for i in range(0, len(seq) - n + 1, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chunked_train(train_seqs: list[str], n: int) -> Path:\n",
    "    \"\"\"Chunk all training sequences and write unique chunks to a temp file.\"\"\"\n",
    "    chunks = set()\n",
    "    for seq in train_seqs:\n",
    "        chunks.update(chunk_sequence(seq, n))\n",
    "    \n",
    "    tmp = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".train\", delete=False, dir=\".\")\n",
    "    tmp.write(\"\\n\".join(chunks) + \"\\n\")\n",
    "    tmp.close()\n",
    "    print(f\"Wrote {len(chunks)} unique chunks (n={n}) to {tmp.name}\")\n",
    "    return Path(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Negative Selection & Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_negsel(train_file: Path, alpha_file: Path, test_chunks: list[str],\n",
    "               n: int, r: int) -> list[float]:\n",
    "    \"\"\"Run negsel2.jar and return the anomaly score for each test chunk.\"\"\"\n",
    "    cmd = [\n",
    "        \"java\", \"-jar\", str(NEGSEL_JAR),\n",
    "        \"-alphabet\", f\"file://{alpha_file}\",\n",
    "        \"-self\", str(train_file),\n",
    "        \"-n\", str(n),\n",
    "        \"-r\", str(r),\n",
    "        \"-c\", \"-l\",\n",
    "    ]\n",
    "    input_data = \"\\n\".join(test_chunks) + \"\\n\"\n",
    "    result = subprocess.run(cmd, input=input_data, capture_output=True, text=True, timeout=300)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"negsel2 failed: {result.stderr}\")\n",
    "    return [float(x) for x in result.stdout.strip().split(\"\\n\") if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequences(train_file: Path, alpha_file: Path, test_seqs: list[str],\n",
    "                    n: int, r: int) -> np.ndarray:\n",
    "    \"\"\"Score each test sequence: chunk it, score each chunk, average per sequence.\"\"\"\n",
    "    all_chunks = []\n",
    "    seq_indices = []\n",
    "    for i, seq in enumerate(test_seqs):\n",
    "        chunks = chunk_sequence(seq, n)\n",
    "        if not chunks:\n",
    "            continue  # sequence shorter than n, gets score 0\n",
    "        all_chunks.extend(chunks)\n",
    "        seq_indices.extend([i] * len(chunks))\n",
    "    \n",
    "    chunk_scores = run_negsel(train_file, alpha_file, all_chunks, n, r)\n",
    "    \n",
    "    seq_scores = np.zeros(len(test_seqs))\n",
    "    seq_counts = np.zeros(len(test_seqs))\n",
    "    for idx, score in zip(seq_indices, chunk_scores):\n",
    "        seq_scores[idx] += score\n",
    "        seq_counts[idx] += 1\n",
    "    seq_scores /= np.maximum(seq_counts, 1)\n",
    "    return seq_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC / AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, scores, title=\"\"):\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n",
    "    plt.xlabel(\"1 - Specificity (FPR)\")\n",
    "    plt.ylabel(\"Sensitivity (TPR)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Experiment (n=10, r=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "R = 4\n",
    "results = {}\n",
    "\n",
    "for dataset_name in [\"snd-cert\", \"snd-unm\"]:\n",
    "    train_seqs, alpha_file, test_sets = load_dataset(dataset_name)\n",
    "    train_file = write_chunked_train(train_seqs, N)\n",
    "    \n",
    "    try:\n",
    "        for test_idx, (test_seqs, labels) in enumerate(test_sets, 1):\n",
    "            print(f\"\\nScoring {dataset_name} test {test_idx} (n={N}, r={R})...\")\n",
    "            scores = score_sequences(train_file, alpha_file, test_seqs, N, R)\n",
    "            auc = plot_roc(\n",
    "                np.array(labels), scores,\n",
    "                title=f\"{dataset_name} test {test_idx} (n={N}, r={R})\"\n",
    "            )\n",
    "            print(f\"  AUC = {auc:.4f}\")\n",
    "            results[(dataset_name, test_idx)] = auc\n",
    "    finally:\n",
    "        Path(train_file).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Sweep\n",
    "\n",
    "Vary `n` and `r` to find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = [7, 10, 15]\n",
    "r_values = [2, 3, 4, 5, 6]\n",
    "\n",
    "# Sweep on snd-unm test 1 (smallest, fastest)\n",
    "dataset_name = \"snd-unm\"\n",
    "train_seqs, alpha_file, test_sets = load_dataset(dataset_name)\n",
    "test_seqs, labels = test_sets[0]\n",
    "\n",
    "sweep_results = {}\n",
    "\n",
    "for n in n_values:\n",
    "    train_file = write_chunked_train(train_seqs, n)\n",
    "    try:\n",
    "        for r in r_values:\n",
    "            if r >= n:\n",
    "                continue\n",
    "            print(f\"n={n}, r={r}...\", end=\" \")\n",
    "            try:\n",
    "                scores = score_sequences(train_file, alpha_file, test_seqs, n, r)\n",
    "                auc = roc_auc_score(labels, scores)\n",
    "                print(f\"AUC={auc:.4f}\")\n",
    "                sweep_results[(n, r)] = auc\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        Path(train_file).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sweep_results:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for n in n_values:\n",
    "        rs = sorted([r for (nn, r) in sweep_results if nn == n])\n",
    "        aucs = [sweep_results[(n, r)] for r in rs]\n",
    "        if rs:\n",
    "            ax.plot(rs, aucs, \"o-\", label=f\"n={n}\")\n",
    "    ax.set_xlabel(\"r\")\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    ax.set_title(f\"Parameter Sweep: {dataset_name}\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set best parameters from the sweep above\n",
    "BEST_N = 10\n",
    "BEST_R = 4\n",
    "\n",
    "# Re-run on all test sets with the best parameters and produce final ROC plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion\n",
    "\n",
    "**TODO:** Answer the following in the report:\n",
    "\n",
    "1. What preprocessing choices did you make (chunking strategy, overlap vs non-overlap) and why?\n",
    "2. How did you aggregate chunk-level scores into sequence-level anomaly scores?\n",
    "3. What parameters `n` and `r` worked best and why?\n",
    "4. How well does the classifier perform on each dataset (`snd-cert` vs `snd-unm`)?\n",
    "5. What would be the biggest challenges in implementing the negative selection algorithm yourself?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}