{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Intrusion Detection for Unix Processes\n",
    "\n",
    "Apply the negative selection algorithm to detect anomalous system call sequences.\n",
    "\n",
    "**Datasets:** `snd-cert` and `snd-unm` (in `syscalls/`)\n",
    "\n",
    "**Key differences from Exercise 1:**\n",
    "- Sequences are variable-length â†’ must chunk into fixed-length substrings\n",
    "- Labels are in separate `.labels` files (0 = normal, 1 = anomalous)\n",
    "- Must specify `-alphabet file://<alpha_file>` since the alphabet differs from the training data characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".\")\n",
    "NEGSEL_JAR = BASE_DIR / \"negsel2.jar\"\n",
    "SYSCALLS_DIR = BASE_DIR / \"syscalls\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str):\n",
    "    \"\"\"Load a syscalls dataset (e.g. 'snd-cert' or 'snd-unm').\n",
    "    \n",
    "    Returns:\n",
    "        train_seqs: list of training sequences\n",
    "        alpha_file: path to alphabet file\n",
    "        test_sets: list of (test_seqs, labels) tuples for each .test/.labels pair\n",
    "    \"\"\"\n",
    "    d = SYSCALLS_DIR / name\n",
    "    \n",
    "    train_file = d / f\"{name}.train\"\n",
    "    alpha_file = d / f\"{name}.alpha\"\n",
    "    train_seqs = train_file.read_text().strip().split(\"\\n\")\n",
    "    \n",
    "    test_sets = []\n",
    "    for i in [1, 2, 3]:\n",
    "        test_file = d / f\"{name}.{i}.test\"\n",
    "        label_file = d / f\"{name}.{i}.labels\"\n",
    "        test_seqs = test_file.read_text().strip().split(\"\\n\")\n",
    "        labels = [int(x) for x in label_file.read_text().strip().split(\"\\n\")]\n",
    "        test_sets.append((test_seqs, labels))\n",
    "    \n",
    "    return train_seqs, alpha_file, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"snd-cert\", \"snd-unm\"]:\n",
    "    train, alpha, tests = load_dataset(name)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"  Alphabet: {alpha.read_text().strip()[:60]}...\")\n",
    "    print(f\"  Training sequences: {len(train)}\")\n",
    "    train_lens = [len(s) for s in train]\n",
    "    print(f\"  Train seq lengths: min={min(train_lens)}, max={max(train_lens)}, median={int(np.median(train_lens))}\")\n",
    "    for i, (seqs, labels) in enumerate(tests, 1):\n",
    "        n_anom = sum(labels)\n",
    "        seq_lens = [len(s) for s in seqs]\n",
    "        print(f\"  Test {i}: {len(seqs)} seqs ({n_anom} anomalous, {len(seqs)-n_anom} normal), \"\n",
    "              f\"lengths: min={min(seq_lens)}, max={max(seq_lens)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Chunking Sequences\n",
    "\n",
    "Since sequences are variable-length, we need to split them into fixed-length chunks of size `n` for both training and testing.\n",
    "\n",
    "**Strategy choices:**\n",
    "- Overlapping (sliding window) vs. non-overlapping chunks\n",
    "- What to do with the remainder (discard if shorter than `n`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_sequence(seq: str, n: int, overlap: bool = True) -> list[str]:\n",
    "    \"\"\"Split a sequence into chunks of length n.\n",
    "    \n",
    "    Args:\n",
    "        seq: input sequence string\n",
    "        n: chunk length\n",
    "        overlap: if True, use a sliding window (stride=1); otherwise non-overlapping (stride=n)\n",
    "    \n",
    "    Returns:\n",
    "        list of chunks of exactly length n\n",
    "    \"\"\"\n",
    "    stride = 1 if overlap else n\n",
    "    chunks = []\n",
    "    for i in range(0, len(seq) - n + 1, stride):\n",
    "        chunks.append(seq[i:i+n])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "example = \"ABCDEFGHIJ\"\n",
    "print(\"Overlapping (n=4):\", chunk_sequence(example, 4, overlap=True))\n",
    "print(\"Non-overlapping (n=4):\", chunk_sequence(example, 4, overlap=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Chunked Training Data\n",
    "\n",
    "Write chunked training sequences to a temporary file that `negsel2.jar` can consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "def write_chunked_train(train_seqs: list[str], n: int, overlap: bool = True) -> Path:\n",
    "    \"\"\"Chunk all training sequences and write to a temp file (one chunk per line).\"\"\"\n",
    "    chunks = []\n",
    "    for seq in train_seqs:\n",
    "        chunks.extend(chunk_sequence(seq, n, overlap))\n",
    "    # Deduplicate to reduce training time\n",
    "    chunks = list(set(chunks))\n",
    "    \n",
    "    tmp = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".train\", delete=False, dir=\".\")\n",
    "    tmp.write(\"\\n\".join(chunks) + \"\\n\")\n",
    "    tmp.close()\n",
    "    print(f\"Wrote {len(chunks)} unique chunks (n={n}) to {tmp.name}\")\n",
    "    return Path(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running Negative Selection\n",
    "\n",
    "Run `negsel2.jar` on chunked test data and aggregate scores per original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_negsel(train_file: Path, alpha_file: Path, test_chunks: list[str],\n",
    "               n: int, r: int, use_log: bool = True) -> list[float]:\n",
    "    \"\"\"Run negsel2.jar and return the anomaly score for each test chunk.\"\"\"\n",
    "    cmd = [\n",
    "        \"java\", \"-jar\", str(NEGSEL_JAR),\n",
    "        \"-alphabet\", f\"file://{alpha_file}\",\n",
    "        \"-self\", str(train_file),\n",
    "        \"-n\", str(n),\n",
    "        \"-r\", str(r),\n",
    "        \"-c\",\n",
    "    ]\n",
    "    if use_log:\n",
    "        cmd.append(\"-l\")\n",
    "    \n",
    "    input_data = \"\\n\".join(test_chunks) + \"\\n\"\n",
    "    result = subprocess.run(cmd, input=input_data, capture_output=True, text=True, timeout=300)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"negsel2 failed: {result.stderr}\")\n",
    "    \n",
    "    scores = [float(x) for x in result.stdout.strip().split(\"\\n\") if x.strip()]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequences(train_file: Path, alpha_file: Path, test_seqs: list[str],\n",
    "                    n: int, r: int, overlap: bool = True) -> np.ndarray:\n",
    "    \"\"\"Compute a composite anomaly score for each variable-length test sequence.\n",
    "    \n",
    "    Chunks each sequence, runs negsel on all chunks, then averages chunk scores\n",
    "    per original sequence.\n",
    "    \"\"\"\n",
    "    # Build flat list of chunks and track which sequence each belongs to\n",
    "    all_chunks = []\n",
    "    seq_indices = []  # maps chunk index -> original sequence index\n",
    "    for i, seq in enumerate(test_seqs):\n",
    "        chunks = chunk_sequence(seq, n, overlap)\n",
    "        if not chunks:\n",
    "            # Sequence shorter than n: use the sequence as-is (padded or skipped)\n",
    "            chunks = [seq]\n",
    "        all_chunks.extend(chunks)\n",
    "        seq_indices.extend([i] * len(chunks))\n",
    "    \n",
    "    # Run negsel on all chunks at once\n",
    "    chunk_scores = run_negsel(train_file, alpha_file, all_chunks, n, r)\n",
    "    \n",
    "    # Aggregate: average chunk scores per sequence\n",
    "    seq_scores = np.zeros(len(test_seqs))\n",
    "    seq_counts = np.zeros(len(test_seqs))\n",
    "    for idx, score in zip(seq_indices, chunk_scores):\n",
    "        seq_scores[idx] += score\n",
    "        seq_counts[idx] += 1\n",
    "    seq_scores /= np.maximum(seq_counts, 1)\n",
    "    \n",
    "    return seq_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC / AUC Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels: np.ndarray, scores: np.ndarray, title: str = \"\"):\n",
    "    \"\"\"Plot ROC curve and return AUC.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n",
    "    plt.xlabel(\"1 - Specificity (FPR)\")\n",
    "    plt.ylabel(\"Sensitivity (TPR)\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment\n",
    "\n",
    "Choose parameters and evaluate on both datasets. Start with the language example parameters (`n=10`, `r=4`) as a baseline, then explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to explore\n",
    "N = 10\n",
    "R = 4\n",
    "OVERLAP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for dataset_name in [\"snd-cert\", \"snd-unm\"]:\n",
    "    train_seqs, alpha_file, test_sets = load_dataset(dataset_name)\n",
    "    \n",
    "    # Prepare chunked training data\n",
    "    train_file = write_chunked_train(train_seqs, N, overlap=OVERLAP)\n",
    "    \n",
    "    try:\n",
    "        for test_idx, (test_seqs, labels) in enumerate(test_sets, 1):\n",
    "            print(f\"\\nScoring {dataset_name} test {test_idx} (n={N}, r={R})...\")\n",
    "            scores = score_sequences(train_file, alpha_file, test_seqs, N, R, overlap=OVERLAP)\n",
    "            auc = plot_roc(\n",
    "                np.array(labels), scores,\n",
    "                title=f\"{dataset_name} test {test_idx} (n={N}, r={R})\"\n",
    "            )\n",
    "            print(f\"  AUC = {auc:.4f}\")\n",
    "            results[(dataset_name, test_idx)] = auc\n",
    "    finally:\n",
    "        Path(train_file).unlink()  # clean up temp file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Parameter Sweep\n",
    "\n",
    "Systematically vary `n` and `r` to find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep over different values of n and r\n",
    "# For each (n, r) combination, compute AUC on one representative test set\n",
    "# and record the results for comparison.\n",
    "\n",
    "n_values = [7, 10, 15]\n",
    "r_values = [2, 3, 4, 5, 6]\n",
    "\n",
    "# Example: sweep on snd-unm test 1 (smallest test set, fastest to evaluate)\n",
    "dataset_name = \"snd-unm\"\n",
    "train_seqs, alpha_file, test_sets = load_dataset(dataset_name)\n",
    "test_seqs, labels = test_sets[0]  # test 1\n",
    "\n",
    "sweep_results = {}\n",
    "\n",
    "for n in n_values:\n",
    "    train_file = write_chunked_train(train_seqs, n, overlap=True)\n",
    "    try:\n",
    "        for r in r_values:\n",
    "            if r >= n:\n",
    "                continue\n",
    "            print(f\"n={n}, r={r}...\", end=\" \")\n",
    "            try:\n",
    "                scores = score_sequences(train_file, alpha_file, test_seqs, n, r)\n",
    "                auc = roc_auc_score(labels, scores)\n",
    "                print(f\"AUC={auc:.4f}\")\n",
    "                sweep_results[(n, r)] = auc\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        Path(train_file).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter sweep results\n",
    "if sweep_results:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for n in n_values:\n",
    "        rs = [r for (nn, r) in sweep_results if nn == n]\n",
    "        aucs = [sweep_results[(n, r)] for r in rs]\n",
    "        if rs:\n",
    "            ax.plot(rs, aucs, \"o-\", label=f\"n={n}\")\n",
    "    ax.set_xlabel(\"r\")\n",
    "    ax.set_ylabel(\"AUC\")\n",
    "    ax.set_title(f\"Parameter Sweep: {dataset_name}\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Evaluation with Best Parameters\n",
    "\n",
    "Use the best `(n, r)` found above to evaluate on all test sets of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set best parameters from the sweep above\n",
    "BEST_N = 10\n",
    "BEST_R = 4\n",
    "\n",
    "# Re-run on all test sets with the best parameters and produce final ROC plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion\n",
    "\n",
    "**TODO:** Answer the following in the report:\n",
    "\n",
    "1. What preprocessing choices did you make (chunking strategy, overlap vs non-overlap) and why?\n",
    "2. How did you aggregate chunk-level scores into sequence-level anomaly scores?\n",
    "3. What parameters `n` and `r` worked best and why?\n",
    "4. How well does the classifier perform on each dataset (`snd-cert` vs `snd-unm`)?\n",
    "5. What would be the biggest challenges in implementing the negative selection algorithm yourself?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}